{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Multi-task Learning & Domain Adaptation\n",
    "##  Named Entity Recognition\n",
    "\n",
    "Today we're gonna solve the problem of named entity recognition. Here's what it does in one picture:\n",
    "![img](https://commons.bmstu.wiki/images/0/00/NER1.png)\n",
    "[image source](https://bit.ly/2Pmg7L2)\n",
    "\n",
    "\n",
    "For each word, in a sentence, your model should predict a named entity class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in colab, uncomment this:\n",
    "# !pip install tensorflow==2.0.0\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "keras, L = tf.keras, tf.keras.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "### Train set\n",
    "\n",
    "Our model will train on a [Groningen Meaning Bank corpus](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus).\n",
    "\n",
    "Each word of every sentence is labelled with named entity class and a part-of-speech tag.\n",
    "\n",
    "### Source domain testset\n",
    "\n",
    "Our train set consists from texts from different news sources. Therefore as source-domain testset we will use data from [CoNLL-2003 Shared Task](https://github.com/Franck-Dernoncourt/NeuroNER/blob/master/data/conll2003/en). More information about the task can be found [here](https://www.clips.uantwerpen.be/conll2003/ner/).\n",
    "\n",
    "### Target domain (in-domain) data\n",
    "\n",
    "As target-domain data we will use data from [WNUT17 Emerging and Rare entity recognition task](http://noisy-text.github.io/2017/emerging-rare-entities.html). This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. The data were mined from mined from Twitter, Reddit,\n",
    "YouTube and StackExchange. Results of different competitors of the task were published [here](https://noisy-text.github.io/2017/pdf/WNUT18.pdf).\n",
    "\n",
    "### Named entity classes\n",
    "\n",
    "* PER - _person_: names of people (e.g. Alexander S. Pushkin)\n",
    "* ORG - _organization_: names of corporations (e.g. Yandex), names of non-profit organizations (e.g. UNICEF)\n",
    "Google).\n",
    "* LOC - _location_ : e.g. Russia\n",
    "* MISC - _miscellaneous_ : other named entities including names of products (e.g. iPhone) and creative works (e.g. Bohemian Rhapsody)\n",
    "\n",
    "### Evaluation metrics\n",
    "\n",
    "As evaluation metrics we will F1 measure on exact matched NEs. It means that partially overlapped enitities of same class are considered as mismatch.\n",
    "For example, LOC entities below is partially overlapped. And it is a mismatch:\n",
    "\n",
    "__O, B-LOC, I-LOC, O__\n",
    "\n",
    "__O, B-LOC, I-LOC, I-LOC__\n",
    "\n",
    "Details can be found in the code of _conlleval.py_\n",
    "\n",
    "### Data format\n",
    "\n",
    "The format of all dataset follows popular [IOB format](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)). The B- prefix before a tag indicates that the tag is the beginning of a chunk, and an I- prefix before a tag indicates that the tag is inside a chunk. The B- tag is used only when a tag is followed by a tag of the same type without O tokens between them. An O tag indicates that a token belongs to no chunk.\n",
    "\n",
    "The named entity labels include:\n",
    "* __B-LOC__ - location - first token\n",
    "* __I-LOC__ - location - subsequent tokens\n",
    "* __B-ORG__ - organization - first token\n",
    "* __O__ - not a named entity\n",
    "\n",
    "Take a look for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train:\n",
    "!wget https://www.dropbox.com/s/xobyz6jgovvz3dm/kaggle-train.conll?dl=1 -O kaggle-train.conll\n",
    "\n",
    "# Source domain testset:\n",
    "!wget https://www.dropbox.com/s/1l8b9iy78cglrw3/source-domain-test.conll?dl=1 -O source-domain-test.conll\n",
    "\n",
    "# Target domain testset:\n",
    "!wget https://www.dropbox.com/s/oxfkdy23ux5hfz5/target-domain-test.conll?dl=1 -O target-domain-test.conll\n",
    "    \n",
    "# Target domain monolingual data:\n",
    "!wget https://www.dropbox.com/s/ysdrotjdfljydbr/target-domain-monolingual.conll?dl=1 -O target-domain-monolingual.conll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conlleval import evaluate\n",
    "from utils import read_conll\n",
    "data = read_conll('./kaggle-train.conll', lower_words=True)\n",
    "\n",
    "data[333]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outdomain_data = read_conll('./source-domain-test.conll', lower_words=True)\n",
    "test_indomain_data = read_conll('./target-domain-test.conll', column_names=['word', 'ne'], lower_words=True)\n",
    "monolingual_indomain_data = read_conll('./target-domain-monolingual.conll', column_names=['word'], lower_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, dev_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "print(\"train: {}, dev: {}\".format(len(train_data), len(dev_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Vocab\n",
    "vocabs = {\n",
    "    key: Vocab.from_lines([row[key] for row in train_data])\n",
    "    for key in ['word', 'pos', 'ne']\n",
    "}\n",
    "\n",
    "def prepare_batch(data):\n",
    "    keys = data[0].keys()\n",
    "    return {\n",
    "        key: vocabs[key].to_matrix(row[key] for row in data)\n",
    "        for key in keys\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_rows = sorted(data, key=lambda row: len(row['word']))[100:102]\n",
    "print(dummy_rows[0])\n",
    "print(dummy_rows[1])\n",
    "prepare_batch(dummy_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: single-task model (1 point)\n",
    "\n",
    "![img](https://github.com/yandexdataschool/nlp_course/raw/master/resources/gorynich_ne.png)\n",
    "\n",
    "Let's start with a straightforward model that does named entity recognition.\n",
    "\n",
    "\n",
    "The image will make sense later :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleModel(L.Layer):\n",
    "    def __init__(self, emb_size=128, hid_size=128):\n",
    "        \"\"\" \n",
    "        A model that predicts named entity class for each word\n",
    "        We recommend the following model:\n",
    "        * Embedding\n",
    "        * Bi-directional LSTM\n",
    "        * Linear layer to predict logits\n",
    "        \"\"\"\n",
    "        super().__init__() # initialize base class to track sub-layers, trainable variables, etc.\n",
    "        \n",
    "        # define layers\n",
    "        self.emb = L.Embedding(len(vocabs['word']), emb_size)\n",
    "        \n",
    "        <YOUR CODE HERE>\n",
    "    \n",
    "    def __call__(self, input_ix):\n",
    "        \"\"\"\n",
    "        Compute logits for named entity recognition\n",
    "        :param input_ix: a matrix of token indices, int32[batch_size, seq_length]\n",
    "        \"\"\"\n",
    "        <YOUR CODE>\n",
    "        return {'ne': ner_logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()\n",
    "\n",
    "dummy_ix = tf.convert_to_tensor(prepare_batch(train_data[:3])['word'])\n",
    "dummy_logits = model(dummy_ix)['ne'].numpy()\n",
    "\n",
    "\n",
    "assert dummy_logits.shape == (3, dummy_ix.shape[1], len(vocabs['ne']))\n",
    "assert dummy_logits.min() < 0 and dummy_logits.max() > 0, \"you ~may~ have added nonlinearity after logits.\"\\\n",
    "                                                          \"Make sure they're just a linear layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import infer_mask\n",
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "def train_step(model, **batch):\n",
    "    \"\"\" A bunch of tensorflow operations used for model training \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(batch['word'])\n",
    "        mask = infer_mask(batch['word'])\n",
    "\n",
    "        loss = -tf.nn.log_softmax(outputs['ne'], -1) * tf.one_hot(batch['ne'], len(vocabs['ne']))\n",
    "        loss = tf.reduce_sum(loss * mask[:, :, None]) / tf.reduce_sum(mask)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Nothin' special: sample random batches and perform SGD steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(data, batch_size=128, shuffle=True, cycle=False, max_batches=None):\n",
    "    indices = np.arange(len(data))\n",
    "    total_batches = 0\n",
    "    while True:\n",
    "        if shuffle: indices = np.random.permutation(indices)\n",
    "        for start_i in range(0, len(data), batch_size):\n",
    "            batch_ix = indices[start_i: start_i + batch_size]\n",
    "            yield prepare_batch(data[batch_ix])\n",
    "            total_batches += 1\n",
    "            if max_batches and total_batches >= max_batches:\n",
    "                return\n",
    "        if not cycle: break\n",
    "            \n",
    "\n",
    "def compute_error_rate(model, data, batch_size=128, key='ne'):\n",
    "    numerator = denominator = 0.0\n",
    "    for batch in iterate_minibatches(data, batch_size, shuffle=False, cycle=False):\n",
    "        batch_ne_logits = model(batch['word'])[key].numpy()\n",
    "        batch_mask = infer_mask(batch['word']).numpy()\n",
    "        \n",
    "        numerator += np.sum((batch[key] == batch_ne_logits.argmax(-1)) * batch_mask)\n",
    "        denominator += batch_mask.sum()\n",
    "    return (1.0 - numerator / denominator) * 100\n",
    "\n",
    "def decode_greedy(model, data, vocabs, batch_size=128, key='ne'):\n",
    "    result = []\n",
    "    for batch in iterate_minibatches(data, batch_size, shuffle=False, cycle=False):\n",
    "        batch_logits = model(batch['word'])[key].numpy()\n",
    "        result.extend(vocabs[key].to_lines(batch_logits.argmax(-1)))\n",
    "    return result\n",
    "\n",
    "def compute_stats(model, data, vocabs, batch_size=128, key='ne', verbose=False):\n",
    "    pred_seqs = decode_greedy(model, data, vocabs, batch_size, key)\n",
    "    true_seqs = [r[key] for r in data]\n",
    "    precision, recall, f1 = evaluate(true_seqs, pred_seqs, verbose)\n",
    "    return precision, recall, f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatsHistory:\n",
    "    def __init__(self):\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "        self.f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "eval_every = 100\n",
    "\n",
    "loss_history = []\n",
    "dev_stats_history = StatsHistory()\n",
    "indomain_stats_history = StatsHistory()\n",
    "outdomain_stats_history = StatsHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for batch in tqdm(iterate_minibatches(train_data, cycle=True, max_batches=2500), total=2500):\n",
    "    loss_t = train_step(model, **batch)\n",
    "    loss_history.append(loss_t)\n",
    "    \n",
    "    if len(loss_history) % eval_every == 0:\n",
    "        clear_output(True)\n",
    "        precision, recall, f1 = compute_stats(model, dev_data, vocabs, verbose=True)\n",
    "        dev_stats_history.precision.append(precision)\n",
    "        dev_stats_history.recall.append(recall)\n",
    "        dev_stats_history.f1.append(f1)\n",
    "        \n",
    "        _, _, f1 = compute_stats(model, test_outdomain_data, vocabs, verbose=True)\n",
    "        outdomain_stats_history.f1.append(f1)\n",
    "        \n",
    "        _, _, f1 = compute_stats(model, test_indomain_data, vocabs, verbose=True)\n",
    "        indomain_stats_history.f1.append(f1)\n",
    "\n",
    "        plt.figure(figsize=[12, 6])\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(loss_history)\n",
    "        plt.title('train loss'), plt.grid()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        \n",
    "        plt.plot(np.arange(1, len(dev_stats_history.f1) + 1) * eval_every, dev_stats_history.f1, label=\"dev f1\")\n",
    "        plt.plot(np.arange(1, len(outdomain_stats_history.f1) + 1) * eval_every, outdomain_stats_history.f1, label=\"outdomain f1\")\n",
    "        plt.plot(np.arange(1, len(indomain_stats_history.f1) + 1) * eval_every, indomain_stats_history.f1, label=\"indomain f1\")\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title('dev stats %'), plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best dev f1 = %.3f%%\" % max(dev_stats_history.f1),\n",
    "      \"\\nBest in-domain f1 = %.3f%%\" % max(indomain_stats_history.f1),\n",
    "      \"\\nBest out-of-domain f1 = %.3f%%\" % max(outdomain_stats_history.f1))\n",
    "assert max(dev_stats_history.f1) > 75, \"you can do better\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multitask model: NER + POS (2 points)\n",
    "\n",
    "Our data contains not only named entity labels, but also part-of-speech tags. Those problems are similar in nature, making it a good candidate for multi-tasking. With any luck, ouyr model will become better at named entity recognition by learning for POS-tagging.\n",
    "\n",
    "![model2](https://github.com/yandexdataschool/nlp_course/raw/master/resources/gorynich_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTaskModel(L.Layer):\n",
    "    def __init__(self, emb_size=128, hid_size=128):\n",
    "        \"\"\" \n",
    "        Equivalent to the SimpleModel above, but with two \n",
    "        linear \"heads\": one for \"ne\" logits and another for \"pos\".\n",
    "        Both heads should grow from the same intermediate \"body\" layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        <YOUR CODE>\n",
    "\n",
    "    \n",
    "    def __call__(self, input_ix):\n",
    "        \"\"\"\n",
    "        Compute logits for named entity recognition and part-of-speech tagging\n",
    "        \"\"\"\n",
    "        <YOUR CODE>\n",
    "        ner_logits = <YOUR_CODE>\n",
    "        pos_logits = <YOUR_CODE>\n",
    "        return {'ne': ner_logits, 'pos': pos_logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoTaskModel()\n",
    "\n",
    "dummy_ix = tf.convert_to_tensor(prepare_batch(train_data[:3])['word'])\n",
    "dummy_out = model(dummy_ix)\n",
    "assert 'ne' in dummy_out and 'pos' in dummy_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import infer_mask\n",
    "\n",
    "def train_step_two_task(model, tasks=('ne', 'pos'), **batch):\n",
    "    \"\"\" Naive train step for two-task loss: ne and pos \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # model predictions\n",
    "        <YOUR CODE>\n",
    "\n",
    "        # losses for each task\n",
    "        <YOUR CODE>\n",
    "    \n",
    "    # compute and apply gradients\n",
    "    <YOUR CODE>\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "loss_history = []\n",
    "dev_stats_history = StatsHistory()\n",
    "indomain_stats_history = StatsHistory()\n",
    "outdomain_stats_history = StatsHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tqdm(iterate_minibatches(train_data, cycle=True, max_batches=2500), total=2500):\n",
    "    loss_t = train_step_two_task(model, **batch)\n",
    "    loss_history.append(loss_t)\n",
    "        \n",
    "    if len(loss_history) % 100 == 0:\n",
    "        precision, recall, f1 = compute_stats(model, dev_data, vocabs, verbose=True)\n",
    "        dev_stats_history.precision.append(precision)\n",
    "        dev_stats_history.recall.append(recall)\n",
    "        dev_stats_history.f1.append(f1)\n",
    "        \n",
    "        clear_output(True)\n",
    "        \n",
    "        _, _, f1 = compute_stats(model, test_outdomain_data, vocabs, verbose=True)\n",
    "        outdomain_stats_history.f1.append(f1)\n",
    "        \n",
    "        _, _, f1 = compute_stats(model, test_indomain_data, vocabs, verbose=True)\n",
    "        indomain_stats_history.f1.append(f1)\n",
    "        \n",
    "        plt.figure(figsize=[12, 6])\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(loss_history)\n",
    "        plt.title('train loss'), plt.grid()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        \n",
    "        plt.plot(np.arange(1, len(dev_stats_history.f1) + 1) * eval_every, dev_stats_history.f1, label=\"dev f1\")\n",
    "        plt.plot(np.arange(1, len(outdomain_stats_history.f1) + 1) * eval_every, outdomain_stats_history.f1, label=\"outdomain f1\")\n",
    "        plt.plot(np.arange(1, len(indomain_stats_history.f1) + 1) * eval_every, indomain_stats_history.f1, label=\"indomain f1\")\n",
    "        \n",
    "        plt.title('dev stats %'), plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best dev f1 = %.3f%%\" % max(dev_stats_history.f1),\n",
    "      \"\\nBest indomain f1 = %.3f%%\" % max(indomain_stats_history.f1),\n",
    "      \"\\nBest outdomain f1 = %.3f%%\" % max(outdomain_stats_history.f1))\n",
    "assert max(dev_stats_history.f1) > 75, \"you can do better\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multitask model: NER + POS + LM (3 points)\n",
    "\n",
    "Two heads are great, but three's even better! Let's add language modeling to the task.\n",
    "\n",
    "With language models, however, there are a few complications:\n",
    "* Our data is too small for LM training. Let's use [1 billion word benchmark](http://www.statmt.org/lm-benchmark/) instead. It *may* even be a good idea to preserve cases.\n",
    "* Language models have some issues with being bidirectional. We recommend training forward and backward models separately and fusing them together. Or use the same approach as [ELMO](https://tfhub.dev/google/elmo/2).\n",
    "* The simplest scheme is to pre-train as a language model and fine-tune for NER and POS. We recommend starting from that.\n",
    "\n",
    "__IMPORTANT!__ NER/POS dataset comes pre-tokenized.  Make sure you apply {almost} the same tokenization when training language model. Alternatively, you can re-tokenize ner/pos data.\n",
    "\n",
    "\n",
    "![model3](https://github.com/yandexdataschool/nlp_course/raw/master/resources/gorynich_small.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskModel(L.Layer):\n",
    "    def __init__(self, name, tasks=('ne', 'pos', 'lm'), emb_size=128, hid_size=128):\n",
    "        \"\"\" \n",
    "        Equivalent to the SimpleModel above, but with three\n",
    "        linear \"heads\": one for \"ne\" logits, second for \"pos\" and last for language modelling ('lm').\n",
    "        Both heads should grow from the same intermediate \"body\" layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.tasks = tasks\n",
    "        # define layers:\n",
    "        <YOUR CODE>\n",
    "\n",
    "    def __call__(self, input_ix):\n",
    "        \"\"\"\n",
    "        Compute logits for all tasks\n",
    "        \"\"\"\n",
    "        <YOUR CODE>\n",
    "\n",
    "        return <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultitaskModel('mod1')\n",
    "\n",
    "dummy_ix = tf.convert_to_tensor(prepare_batch(train_data[:3])['word'])\n",
    "dummy_out = model(dummy_ix)\n",
    "assert 'ne' in dummy_out and 'pos' in dummy_out and 'lm' in dummy_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different schemes for multitask learning with Language Model component.\n",
    "\n",
    "You can try at least two of them:\n",
    "* Pretrain network using monolingual data as Language Model and then train model as NER and POS tagger.\n",
    "* Train network alternately: one step on NER and POS tasks, one step on LM tasks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_multitask(model, tasks=('ne', 'pos', 'lm'), **batch):\n",
    "    \"\"\" Naive train step for two-task loss: ne and pos \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # model predictions\n",
    "        <YOUR CODE>\n",
    "\n",
    "        # losses for each task\n",
    "        <YOUR CODE>\n",
    "    \n",
    "    # compute and apply gradients\n",
    "    <YOUR CODE>\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "loss_history = []\n",
    "dev_stats_history = StatsHistory()\n",
    "indomain_stats_history = StatsHistory()\n",
    "outdomain_stats_history = StatsHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tqdm(iterate_minibatches(train_data, cycle=True, max_batches=2500), total=2500):\n",
    "    # YOUR CODE HERE\n",
    "    loss_history.append(loss_t)\n",
    "    \n",
    "    if len(loss_history) % 100 == 0:\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=[12, 6])\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(loss_history)\n",
    "        plt.title('train loss'), plt.grid()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        \n",
    "        plt.plot(np.arange(1, len(dev_stats_history.f1) + 1) * eval_every, dev_stats_history.f1, label=\"dev f1\")\n",
    "        plt.plot(np.arange(1, len(outdomain_stats_history.f1) + 1) * eval_every, outdomain_stats_history.f1, label=\"outdomain f1\")\n",
    "        plt.plot(np.arange(1, len(indomain_stats_history.f1) + 1) * eval_every, indomain_stats_history.f1, label=\"indomain f1\")\n",
    "        \n",
    "        plt.title('dev stats %'), plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final task: transfer learning from BERT (4++ points)\n",
    "\n",
    "By now you have mined pretty much everything from the CONLL dataset, so now it's time to go beyond.\n",
    "\n",
    "You can __fine-tune a pre-trained BERT__ to solve all three CONLL-based tasks.\n",
    "\n",
    "In case you forgot, BERT is a huge transformer-based model that learns to solve several tasks (e.g. missing word imputation) on a huge dataset.\n",
    "* How bert works: [arxiv](https://arxiv.org/abs/1810.04805)\n",
    "* Bert in TF2.0: [colab notebook](https://colab.research.google.com/drive/1EJuMPW7TDVDGB1wDCIayx22jutcwLQlE)\n",
    "\n",
    "You can also try BERT's friends for fun, swag and bonus points: [RoBERTa](https://arxiv.org/abs/1907.11692), [ALBERT](https://arxiv.org/abs/1909.11942), [XLNet](https://arxiv.org/abs/1906.08237), and even [T5](https://arxiv.org/abs/1910.10683)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A whole lot of your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Bonus) Structured prediction for NEs (3 points)\n",
    "\n",
    "![lstm_crf_ner](https://github.com/yandexdataschool/nlp_course/raw/master/resources/lstm_crf_ner.png)\n",
    "\n",
    "[_Picture from  Lample et al._](https://arxiv.org/abs/1603.01360)\n",
    "\n",
    "A setup with seq2seq and cross-entropy loss for tagging is not so good. Because in the case tagging there is an important constraint: input and output sequence have same length, and the number of output token types is much less than the number of input token types.\n",
    "\n",
    "To use this constraint effectively it is good idea to try structured [CRF loss](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/crf) for NER (and POS)\n",
    "\n",
    "\n",
    "A good example of TensorFlow implementation of NER NN with CRF loss can be found in the [blogpost](https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Bonus) Dealing with letter case and rare entities (2 points)\n",
    "\n",
    "![lstm_crf_ner](https://github.com/yandexdataschool/nlp_course/raw/master/resources/word_and_char_embedding_concat.png)\n",
    "\n",
    "[_Picture from  Lample et al._](https://arxiv.org/abs/1603.01360)\n",
    "\n",
    "First, in European languages (both English and Russian) personal names, companies, geographical names are capitalized traditionally. Thus the letter case carries a powerful signal for named entities recognition. So it is good to utilize it.\n",
    "\n",
    "Second, most of named entities are rare words. In testsets (both from source and target distributions) they are replaced by _UNK_. To deal with OOV words you can try different approaches (feel free!). For example:\n",
    "\n",
    "* You can use additional character-level recurrent layers to obtain character-aware word embeddings (see scheme on the picture above)\n",
    "* You can use pretrained embeddings with character ngram information ([FastText](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md))\n",
    "* You can split words by subword units using [BPE](https://arxiv.org/abs/1508.07909)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (bonus) Domain adaptation via proxy-labels (3 points)\n",
    "As you can see above the quality of NER on the target domain (internet comments) is much worse than on the source domain (news). This is not surprising.\n",
    "\n",
    "To overcome the problem we offer you to implement any kind of proxy-label method. A good overview on this kind of methods can be found [here](https://arxiv.org/abs/1804.09530)\n",
    "\n",
    "__ATTENTION!!!__ For proxy-labeling use monolingual target-domain dataset (not testset!).\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
