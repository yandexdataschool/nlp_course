### Word embeddings
- [__lecture slides__](../resources/slides/nlp19_01_word_embeddings.pdf)
- Our videos: [__intro__](https://yadi.sk/i/BNTJG-_rwf20Gw), [__lecture__](https://yadi.sk/i/nUiHl4VPMOCz0g), [__seminar__](https://yadi.sk/i/QTcGA5mgdhS8jg)
- Lecture video from Stanford CS224N - [__intro__](https://www.youtube.com/watch?v=OQQ-W_63UgQ), [__embeddings__](https://www.youtube.com/watch?v=ERibwqs9p38) (english)


### Practice & homework
The practice for this week takes place in notebooks. Just open them and follow instructions from there.
* __Seminar:__ `./seminar.ipynb`
* __Homework:__ `./homework.ipynb`

Unless explicitly said otherwise, all subsequent weeks follow the same pattern (notebook with instructions).

If you have any difficulties with notebooks, just open them in [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yandexdataschool/nlp_course/blob/master/week01_embeddings/seminar.ipynb).

#### Embedding space walk
![embedding_space_walk](https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/embedding_space_walk.gif)

### More materials (optional)
* On hierarchical & sampled softmax estimation for word2vec [page](http://ruder.io/word-embeddings-softmax/)
* GloVe project [page](https://nlp.stanford.edu/projects/glove/)
* FastText project [repo](https://github.com/facebookresearch/fastText)
* Semantic change over time - oberved through word embeddings - [arxiv](https://arxiv.org/pdf/1605.09096.pdf)
* Another cool link that you could have shared, but decided to hesitate. Or did you?

### Related articles

#### Starting Point

- **Distributed Representations of Words and Phrases and their Compositionality** Mikolov et al., 2013 [[arxiv]](https://arxiv.org/abs/1310.4546)

- **Efficient Estimation of Word Representations in Vector Space** Mikolov et al., 2013 [[arxiv]](https://arxiv.org/abs/1301.3781)

- **Distributed Representations of Sentences and Documents** Quoc Le et al., 2014 [[arxiv]](https://arxiv.org/abs/1405.4053)

- **GloVe: Global Vectors for Word Representation** Pennington et al., 2014 [[article]](https://www.aclweb.org/anthology/D14-1162)

- **Enriching word vectors with subword information** Bojanowski et al., 2016 [[arxiv]](https://arxiv.org/abs/1607.04606) (FastText)


#### Explaination and Analysis

- **Word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method** Yoav Goldberg, Omer Levy, 2014 [[arxiv]](https://arxiv.org/abs/1402.3722)

- **Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors** Marco Baroni, Georgiana Dinu, Germa ́n Kruszewski, ACL 2014, [[paper]](https://www.aclweb.org/anthology/P14-1023)

- **The strange geometry of skip-gram with negative sampling**, David Mimno, Laure Thompson, EMNLP 2017, [[paper]](https://www.aclweb.org/anthology/D17-1308)

- **Characterizing Departures from Linearity in Word Translation** Ndapa Nakashole, Raphael Flauger, ACL 2018, [[paper]](https://aclweb.org/anthology/P18-2036)

- **On the Dimensionality of Word Embedding** Zi Yin, Yuanyuan Shen, NIPS 2018, [[arxiv]](https://arxiv.org/pdf/1812.04224.pdf)

- **Analogies Explained: Towards Understanding Word Embeddings** Carl Allen, Timothy Hospedales, ICML 2019, [[arxiv]](https://arxiv.org/abs/1901.09813) [[official blog post]](https://carl-allen.github.io/nlp/2019/07/01/explaining-analogies-explained.html)



#### Multilingual Embeddings. Unsupervised MT.

- **Exploiting similarities between languages for machine translation** Mikolov et al., 2013 [[arxiv]](https://arxiv.org/abs/1309.4168)

- **Improving vector space word representations using multilingual correlation** Faruqui and Dyer, EACL 2014 [[pdf]](https://www.aclweb.org/anthology/E14-1049)

- **Learning principled bilingual mappings of word embeddings while preserving monolingual invariance** Artetxe et al., EMNLP 2016 [[pdf]](https://aclweb.org/anthology/D16-1250)

- **Offline bilingual word vectors, orthogonal transformations and the inverted softmax** [[arxiv]](https://arxiv.org/abs/1702.03859)
Smith et al., ICLR 2017

- **Word Translation Without Parallel Data** Conneau et al., 2018 [[arxiv]](https://arxiv.org/abs/1710.04087)

