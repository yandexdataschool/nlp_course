### Word embeddings
- [__lecture slides__](https://github.com/yandexdataschool/nlp_course/blob/master/resources/slides/lecture1_word_embeddings.pdf)
- Our videos: [course intro](https://yadi.sk/i/Ff1jVAODd4P9ug), [lecture](https://yadi.sk/i/wzVA1XYKS2u6NQ), [seminar](https://yadi.sk/i/X5UYALfyyrwzYw)
- Lecture video from Stanford CS224N - [__intro__](https://www.youtube.com/watch?v=OQQ-W_63UgQ), [__embeddings__](https://www.youtube.com/watch?v=ERibwqs9p38) (english)


### Practice & homework
The practice for this week takes place in notebooks. Just open them and follow instructions from there.
* __Seminar:__ `./seminar.ipynb`
* __Homework:__ `./homework.ipynb`

Unless explicitly said otherwise, all subsequent weeks follow the same pattern (notebook with instructions).

If you have any difficulties with notebooks, just open them in [colab](https://colab.research.google.com/github/yandexdataschool/nlp_course/blob/master/week01_embeddings/seminar.ipynb).

#### Embedding space walk
![embedding_space_walk](https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/embedding_space_walk.gif)

### More materials (optional)
* On hierarchical & sampled softmax estimation for word2vec [page](http://ruder.io/word-embeddings-softmax/)
* GloVe project [page](https://nlp.stanford.edu/projects/glove/)
* FastText project [repo](https://github.com/facebookresearch/fastText)
* Semantic change over time - oberved through word embeddings - [arxiv](https://arxiv.org/pdf/1605.09096.pdf)
* Another cool link that you could have shared, but decided to hesitate. Or did you?

### Related articles

#### Starting Point

- **Distributed Representations of Words and Phrases and their Compositionality** Mikolov et al., 2013 [[arxiv]](https://arxiv.org/abs/1310.4546)

- **Efficient Estimation of Word Representations in Vector Space** Mikolov et al., 2013 [[arxiv]](https://arxiv.org/abs/1301.3781)

- **Distributed Representations of Sentences and Documents** Quoc Le et al., 2014 [[arxiv]](https://arxiv.org/abs/1405.4053)

- **GloVe: Global Vectors for Word Representation** Pennington et al., 2014 [[article]](https://www.aclweb.org/anthology/D14-1162)

- **Word2vec Explained: Deriving Mikolov et al.â€™s Negative-Sampling Word-Embedding Method** Yoav Goldberg, Omer Levy, 2014 [[arxiv]](https://arxiv.org/abs/1402.3722)


#### Multilingual Embeddings. Unsupervised MT.

- **Enriching word vectors with subword information** Bojanowski et al., 2016 [[arxiv]](https://arxiv.org/abs/1607.04606)

- **Exploiting similarities between languages for machine translation** Mikolov et al., 2013 [[arxiv]](https://arxiv.org/abs/1309.4168)

- **Improving vector space word representations using multilingual correlation** Faruqui and Dyer, EACL 2014 [[pdf]](https://www.aclweb.org/anthology/E14-1049)

- **Learning principled bilingual mappings of word embeddings while preserving monolingual invariance** Artetxe et al., EMNLP 2016 [[pdf]](https://aclweb.org/anthology/D16-1250)

- **Offline bilingual word vectors, orthogonal transformations and the inverted softmax** [[arxiv]](https://arxiv.org/abs/1702.03859)
Smith et al., ICLR 2017

- **Word Translation Without Parallel Data** Conneau et al., 2018 [[arxiv]](https://arxiv.org/abs/1710.04087)

