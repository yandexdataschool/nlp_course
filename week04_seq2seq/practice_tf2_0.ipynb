{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "practive_tf2.0.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "edk_oVg0lrtW"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmsFABwClrsS",
        "colab_type": "text"
      },
      "source": [
        "## Seminar and homework (10 points total)\n",
        "\n",
        "Today we shall compose encoder-decoder neural networks and apply them to the task of machine translation.\n",
        "\n",
        "![img](https://esciencegroup.files.wordpress.com/2016/03/seq2seq.jpg)\n",
        "_(img: esciencegroup.files.wordpress.com)_\n",
        "\n",
        "\n",
        "Encoder-decoder architectures are about converting anything to anything, including\n",
        " * Machine translation and spoken dialogue systems\n",
        " * [Image captioning](http://mscoco.org/dataset/#captions-challenge2015) and [image2latex](https://openai.com/requests-for-research/#im2latex) (convolutional encoder, recurrent decoder)\n",
        " * Generating [images by captions](https://arxiv.org/abs/1511.02793) (recurrent encoder, convolutional decoder)\n",
        " * Grapheme2phoneme - convert words to transcripts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4N9AD2dlrsU",
        "colab_type": "text"
      },
      "source": [
        "## Our task: machine translation\n",
        "\n",
        "We gonna try our encoder-decoder models on russian to english machine translation problem. More specifically, we'll translate hotel and hostel descriptions. This task shows the scale of machine translation while not requiring you to train your model for weeks if you don't use GPU.\n",
        "\n",
        "Before we get to the architecture, there's some preprocessing to be done. ~~Go tokenize~~ Alright, this time we've done preprocessing for you. As usual, the data will be tokenized with WordPunctTokenizer.\n",
        "\n",
        "However, there's one more thing to do. Our data lines contain unique rare words. If we operate on a word level, we will have to deal with large vocabulary size. If instead we use character-level models, it would take lots of iterations to process a sequence. This time we're gonna pick something inbetween.\n",
        "\n",
        "One popular approach is called [Byte Pair Encoding](https://github.com/rsennrich/subword-nmt) aka __BPE__. The algorithm starts with a character-level tokenization and then iteratively merges most frequent pairs for N iterations. This results in frequent words being merged into a single token and rare words split into syllables or even characters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfvojjHQlrsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install tensorflow-gpu==2.0.0\n",
        "!pip3 install subword-nmt &> log\n",
        "!wget https://www.dropbox.com/s/yy2zqh34dyhv07i/data.txt?dl=1 -O data.txt\n",
        "!wget https://www.dropbox.com/s/fj9w01embfxvtw1/dummy_checkpoint.npz?dl=1 -O dummy_checkpoint.npz\n",
        "!wget https://raw.githubusercontent.com/yandexdataschool/nlp_course/2019/week04_seq2seq/utils.py -O utils.py\n",
        "# thanks to tilda and deephack teams for the data, Dmitry Emelyanenko for the code :)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9kP0SdxlrsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from subword_nmt.learn_bpe import learn_bpe\n",
        "from subword_nmt.apply_bpe import BPE\n",
        "tokenizer = WordPunctTokenizer()\n",
        "def tokenize(x):\n",
        "    return ' '.join(tokenizer.tokenize(x.lower()))\n",
        "\n",
        "# split and tokenize the data\n",
        "with open('train.en', 'w') as f_src,  open('train.ru', 'w') as f_dst:\n",
        "    for line in open('data.txt'):\n",
        "        src_line, dst_line = line.strip().split('\\t')\n",
        "        f_src.write(tokenize(src_line) + '\\n')\n",
        "        f_dst.write(tokenize(dst_line) + '\\n')\n",
        "\n",
        "# build and apply bpe vocs\n",
        "bpe = {}\n",
        "for lang in ['en', 'ru']:\n",
        "    learn_bpe(open('./train.' + lang), open('bpe_rules.' + lang, 'w'), num_symbols=8000)\n",
        "    bpe[lang] = BPE(open('./bpe_rules.' + lang))\n",
        "    \n",
        "    with open('train.bpe.' + lang, 'w') as f_out:\n",
        "        for line in open('train.' + lang):\n",
        "            f_out.write(bpe[lang].process_line(line.strip()) + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UPW3sV8lrsb",
        "colab_type": "text"
      },
      "source": [
        "### Building vocabularies\n",
        "\n",
        "We now need to build vocabularies that map strings to token ids and vice versa. We're gonna need these fellas when we feed training data into model or convert output matrices into words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmTy_m_olrsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PskgBSxlrsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_inp = np.array(open('./train.bpe.ru').read().split('\\n'))\n",
        "data_out = np.array(open('./train.bpe.en').read().split('\\n'))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_inp, dev_inp, train_out, dev_out = train_test_split(data_inp, data_out, test_size=3000,\n",
        "                                                          random_state=42)\n",
        "for i in range(3):\n",
        "    print('inp:', train_inp[i])\n",
        "    print('out:', train_out[i], end='\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vipg4O61lrsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils import Vocab\n",
        "inp_voc = Vocab.from_lines(train_inp)\n",
        "out_voc = Vocab.from_lines(train_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwOoHfuhlrsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here's how you cast lines into ids and backwards.\n",
        "batch_lines = sorted(train_inp, key=len)[5:10]\n",
        "batch_ids = inp_voc.to_matrix(batch_lines)\n",
        "batch_lines_restored = inp_voc.to_lines(batch_ids)\n",
        "\n",
        "print(\"lines\")\n",
        "print(batch_lines)\n",
        "print(\"\\nwords to ids (0 = bos, 1 = eos):\")\n",
        "print(batch_ids)\n",
        "print(\"\\nback to words\")\n",
        "print(batch_lines_restored)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSYu-MkElrsk",
        "colab_type": "text"
      },
      "source": [
        "Draw source and translation length distributions to estimate the scope of the task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLLl9cSNlrsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=[8, 4])\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"source length\")\n",
        "plt.hist(list(map(len, map(str.split, train_inp))), bins=20);\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"translation length\")\n",
        "plt.hist(list(map(len, map(str.split, train_out))), bins=20);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHWgx34flrsn",
        "colab_type": "text"
      },
      "source": [
        "### Encoder-decoder model\n",
        "\n",
        "The code below contas a template for a simple encoder-decoder model: single GRU encoder/decoder, no attention or anything. This model is implemented for you as a reference and a baseline for your homework assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd_rDRm9lrso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2'), \"Current tf version: {}; required: 2.0.*\".format(tf.__version__)\n",
        "L = tf.keras.layers\n",
        "keras = tf.keras\n",
        "from utils import infer_length, infer_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgfN5-F7lrst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicModel(L.Layer):\n",
        "    def __init__(self, inp_voc, out_voc, emb_size=64, hid_size=128):\n",
        "        \"\"\"\n",
        "        A simple encoder-decoder model\n",
        "        \"\"\"\n",
        "        super().__init__() # initialize base class to track sub-layers, trainable variables, etc.\n",
        "\n",
        "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
        "        self.hid_size = hid_size\n",
        "\n",
        "        self.emb_inp = L.Embedding(len(inp_voc), emb_size)\n",
        "        self.emb_out = L.Embedding(len(out_voc), emb_size)\n",
        "        self.enc0 = L.GRUCell(hid_size)\n",
        "\n",
        "        self.dec_start = L.Dense(hid_size)\n",
        "        self.dec0 = L.GRUCell(hid_size)\n",
        "        self.logits = L.Dense(len(out_voc))\n",
        "\n",
        "    def call(self, inp, out):\n",
        "        initial_state = self.encode(inp)\n",
        "        return self.decode(initial_state, out)\n",
        "\n",
        "    def encode(self, inp, **flags):\n",
        "        \"\"\"\n",
        "        Takes symbolic input sequence, computes initial state\n",
        "        :param inp: matrix of input tokens [batch, time]\n",
        "        :returns: initial decoder state tensors, one or many\n",
        "        \"\"\"\n",
        "        inp_emb = self.emb_inp(inp)\n",
        "        batch_size = inp.shape[0]\n",
        "        \n",
        "        mask = infer_mask(inp, self.inp_voc.eos_ix, dtype=tf.bool)\n",
        "\n",
        "        state = [tf.zeros((batch_size, self.hid_size), tf.float32)]\n",
        "        \n",
        "        for i in tf.range(inp_emb.shape[1]):\n",
        "            output, next_state = self.enc0(inp_emb[:, i], state)\n",
        "            state = [\n",
        "                     tf.where(\n",
        "                         tf.tile(mask[:, i, None],[1,next_tensor.shape[1]]),\n",
        "                         next_tensor, tensor\n",
        "                      ) for tensor, next_tensor in zip(state, next_state)\n",
        "            ]\n",
        "        \n",
        "        dec_start = self.dec_start(state[0])\n",
        "        return [dec_start]\n",
        "\n",
        "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
        "        \"\"\"\n",
        "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
        "        :param prev_state: a list of previous decoder state tensors\n",
        "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
        "        :return: a list of next decoder state tensors, a tensor of logits [batch, n_tokens]\n",
        "        \"\"\"\n",
        "        \n",
        "        <YOUR CODE HERE>\n",
        "                \n",
        "        return new_dec_state, output_logits\n",
        "\n",
        "    def decode(self, initial_state, out_tokens, **flags):\n",
        "        state = initial_state\n",
        "        batch_size = out_tokens.shape[0]\n",
        "\n",
        "        # initial logits: always predict BOS\n",
        "        first_logits = tf.math.log(\n",
        "            tf.one_hot(tf.fill([batch_size], self.out_voc.bos_ix), len(self.out_voc)) + 1e-30)\n",
        "        outputs = [first_logits]\n",
        "\n",
        "        for i in tf.range(out_tokens.shape[1] - 1):\n",
        "          #on each step update the state and store new logits into output\n",
        "          <YOUR CODE HERE>\n",
        "\n",
        "        return tf.stack(outputs, axis=1)\n",
        "\n",
        "    def decode_inference(self, initial_state, max_len=100, **flags):\n",
        "        state = initial_state\n",
        "        outputs = [tf.ones(initial_state[0].shape[0], tf.int32) * self.out_voc.bos_ix]\n",
        "        all_states = initial_state\n",
        "\n",
        "        for i in tf.range(max_len):\n",
        "            state, logits = self.decode_step(state, outputs[-1])\n",
        "            outputs.append(tf.argmax(logits, axis=-1, output_type=tf.int32))\n",
        "            all_states.append(state[0])\n",
        "        \n",
        "        return tf.stack(outputs, axis=1), tf.stack(all_states, axis=1)\n",
        "\n",
        "    def translate_lines(self, inp_lines):\n",
        "      inp = tf.convert_to_tensor(inp_voc.to_matrix(inp_lines))\n",
        "      \n",
        "      initial_state = self.encode(inp)\n",
        "      out_ids, states = self.decode_inference(initial_state)\n",
        "      return out_voc.to_lines(out_ids.numpy()), states\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aGkMAU0BtB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BasicModel(inp_voc, out_voc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wuv1-aVlrs0",
        "colab_type": "text"
      },
      "source": [
        "### Training loss (2 points)\n",
        "\n",
        "Our training objetive is almost the same as it was for neural language models:\n",
        "$$ L = {\\frac1{|D|}} \\sum_{X, Y \\in D} \\sum_{y_t \\in Y} - \\log p(y_t \\mid y_1, \\dots, y_{t-1}, X, \\theta) $$\n",
        "\n",
        "where $|D|$ is the __total length of all sequences__, including BOS and first EOS, but excluding PAD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Cmv9Lrulrs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dummy_inp = tf.convert_to_tensor(inp_voc.to_matrix(train_inp[:3]))\n",
        "dummy_out = tf.convert_to_tensor(out_voc.to_matrix(train_out[:3]))\n",
        "dummy_logits = model(dummy_inp, dummy_out)\n",
        "ref_shape = (dummy_out.shape[0], dummy_out.shape[1], len(out_voc))\n",
        "assert dummy_logits.shape == ref_shape, \"Your logits shape should be {} but got {}\".format(dummy_logits.shape, ref_shape)\n",
        "assert all(dummy_logits[:, 0].numpy().argmax(-1) == out_voc.bos_ix), \"first step must always be BOS\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "c8XPV8sWlrs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils import select_values_over_last_axis\n",
        "\n",
        "\n",
        "def compute_loss(model, inp, out, **flags):\n",
        "    \"\"\"\n",
        "    Compute loss (float32 scalar) as in the formula above\n",
        "    :param inp: input tokens matrix, int32[batch, time]\n",
        "    :param out: reference tokens matrix, int32[batch, time]\n",
        "    \n",
        "    In order to pass the tests, your function should\n",
        "    * include loss at first EOS but not the subsequent ones\n",
        "    * divide sum of losses by a sum of input lengths (use infer_length or infer_mask)\n",
        "    \"\"\"\n",
        "    inp, out = map(tf.convert_to_tensor, [inp, out])\n",
        "    \n",
        "\n",
        "    #outputs of the model\n",
        "    logits_seq = <YOUR CODE HERE>\n",
        "\n",
        "    #log-probabilities of all tokens at all steps\n",
        "    logprobs_seq = <YOUR CODE HERE>\n",
        "   \n",
        "    #select the correct\n",
        "    logp_out = logprobs_seq * tf.one_hot(out, len(model.out_voc), dtype=tf.float32)\n",
        "    mask = infer_mask(out, out_voc.eos_ix)\n",
        "\n",
        "    #return cross-entropy\n",
        "    return <YOUR CODE HERE>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME_LWUeklrs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dummy_loss = compute_loss(model, dummy_inp, dummy_out)\n",
        "print(\"Loss:\", dummy_loss)\n",
        "assert np.allclose(dummy_loss, 8.425, rtol=0.1, atol=0.1), \"We're sorry for your loss\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpbaBpW7lrs-",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation: BLEU\n",
        "\n",
        "Machine translation is commonly evaluated with [BLEU](https://en.wikipedia.org/wiki/BLEU) score. This metric simply computes which fraction of predicted n-grams is actually present in the reference translation. It does so for n=1,2,3 and 4 and computes the geometric average with penalty if translation is shorter than reference.\n",
        "\n",
        "While BLEU [has many drawbacks](http://www.cs.jhu.edu/~ccb/publications/re-evaluating-the-role-of-bleu-in-mt-research.pdf), it still remains the most commonly used metric and one of the simplest to compute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb1-PhKIlrs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "def compute_bleu(model, inp_lines, out_lines, bpe_sep='@@ ', **flags):\n",
        "    \"\"\" Estimates corpora-level BLEU score of model's translations given inp and reference out \"\"\"\n",
        "    translations, _ = model.translate_lines(inp_lines, **flags)#TODO batches!\n",
        "    translations = [line.replace(bpe_sep, '') for line in translations] \n",
        "\n",
        "    return corpus_bleu([[ref] for ref in out_lines], translations) * 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZvfid1RlrtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compute_bleu(model, dev_inp, dev_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQDhGwg4lrtC",
        "colab_type": "text"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "Training encoder-decoder models isn't that different from any other models: sample batches, compute loss, backprop and update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "yfwIaixHlrtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm, trange\n",
        "metrics = {'train_loss': [], 'dev_bleu': [] }\n",
        "\n",
        "opt = keras.optimizers.Adam(1e-3)\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LlDT6eDUlrtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for _ in trange(25000):\n",
        "    step = len(metrics['train_loss']) + 1\n",
        "    batch_ix = np.random.randint(len(train_inp), size=batch_size)\n",
        "    batch_inp = inp_voc.to_matrix(train_inp[batch_ix])\n",
        "    batch_out = out_voc.to_matrix(train_out[batch_ix])\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "         loss_t = compute_loss(model, batch_inp, batch_out)\n",
        "    \n",
        "    grads = tape.gradient(loss_t, model.trainable_variables)\n",
        "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    \n",
        "    \n",
        "    metrics['train_loss'].append((step, loss_t.numpy()))\n",
        "    \n",
        "    if step % 100 == 0:\n",
        "        metrics['dev_bleu'].append((step, compute_bleu(model, dev_inp, dev_out)))\n",
        "        \n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=(12,4))\n",
        "        for i, (name, history) in enumerate(sorted(metrics.items())):\n",
        "            plt.subplot(1, len(metrics), i + 1)\n",
        "            plt.title(name)\n",
        "            plt.plot(*zip(*history))\n",
        "            plt.grid()\n",
        "        plt.show()\n",
        "        print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)\n",
        "        \n",
        "# Note: it's okay if bleu oscillates up and down as long as it gets better on average over long term (e.g. 5k batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ahuhKVhlrtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert np.mean(metrics['dev_bleu'][-10:], axis=0)[1] > 35, \"We kind of need a higher bleu BLEU from you. Kind of right now.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyaHOpealrtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for inp_line, trans_line in zip(dev_inp[::500], model.translate_lines(dev_inp[::500])[0]):\n",
        "    print(inp_line)\n",
        "    print(trans_line)\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv0s8qxOXp5y",
        "colab_type": "text"
      },
      "source": [
        "# Homework code templates will appear here soon!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edk_oVg0lrtW",
        "colab_type": "text"
      },
      "source": [
        "### Your Attention Required (4 points)\n",
        "\n",
        "In this section we want you to improve over the basic model by implementing a simple attention mechanism.\n",
        "\n",
        "This is gonna be a two-parter: building the __attention layer__ and using it for an __attentive seq2seq model__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJtw3VvgcDaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "COMMING SOON!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz9aROAIlrtX",
        "colab_type": "text"
      },
      "source": [
        "### Attention layer\n",
        "\n",
        "Here you will have to implement a layer that computes a simple additive attention:\n",
        "\n",
        "Given encoder sequence $ h^e_0, h^e_1, h^e_2, ..., h^e_T$ and a single decoder state $h^d$,\n",
        "\n",
        "* Compute logits with a 2-layer neural network\n",
        "$$a_t = linear_{out}(tanh(linear_{e}(h^e_t) + linear_{d}(h_d)))$$\n",
        "* Get probabilities from logits, \n",
        "$$ p_t = {{e ^ {a_t}} \\over { \\sum_\\tau e^{a_\\tau} }} $$\n",
        "\n",
        "* Add up encoder states with probabilities to get __attention response__\n",
        "$$ attn = \\sum_t p_t \\cdot h^e_t $$\n",
        "\n",
        "You can learn more about attention layers in the leture slides or [from this post](https://distill.pub/2016/augmented-rnns/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IalfpdAelrtb",
        "colab_type": "text"
      },
      "source": [
        "### Seq2seq model with attention\n",
        "\n",
        "You can now use the attention layer to build a network. The simplest way to implement attention is to use it in decoder phase:\n",
        "![img](https://i.imgur.com/6fKHlHb.png)\n",
        "_image from distill.pub [article](https://distill.pub/2016/augmented-rnns/)_\n",
        "\n",
        "On every step, use __previous__ decoder state to obtain attention response. Then feed concat this response to the inputs of next attetion layer.\n",
        "\n",
        "The key implementation detail here is __model state__. Put simply, you can add any tensor into the list of `encode` outputs. You will then have access to them at each `decode` step. This may include:\n",
        "* Last RNN hidden states (as in basic model)\n",
        "* The whole sequence of encoder outputs (to attend to) and mask\n",
        "* Attention probabilities (to visualize)\n",
        "\n",
        "_There are, of course, alternative ways to wire attention into your network and different kinds of attention. Take a look at [this](https://arxiv.org/abs/1609.08144), [this](https://arxiv.org/abs/1706.03762) and [this](https://arxiv.org/abs/1808.03867) for ideas. And for image captioning/im2latex there's [visual attention](https://arxiv.org/abs/1502.03044)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCKPB5JmcE6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "COMMING SOON!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryZCOTEslrtf",
        "colab_type": "text"
      },
      "source": [
        "### Training attentive model\n",
        "\n",
        "We'll reuse the infrastructure you've built for the regular model. I hope you didn't hard-code anything :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YMHPgZxcFaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "COMMING SOON!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbIIngNVlrtt",
        "colab_type": "text"
      },
      "source": [
        "## Grand Finale (4+ points)\n",
        "\n",
        "We want you to find the best model for the task. Use everything you know.\n",
        "\n",
        "* different recurrent units: rnn/gru/lstm; deeper architectures\n",
        "* bidirectional encoder, different attention methods for decoder\n",
        "* word dropout, training schedules, anything you can imagine\n",
        "\n",
        "For a full grade we want you to conduct at least __two__ experiments from two different bullet-points or your alternative ideas (2 points each). Extra work will be rewarded with bonus points :)\n",
        "\n",
        "As usual, we want you to describe what you tried and what results you obtained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "2rzAj_xtlrtt",
        "colab_type": "text"
      },
      "source": [
        "`[your report/log here or anywhere you please]`"
      ]
    }
  ]
}
